### RETRIEVAL HEAD MECHANISTICALLY EXPLAINS LONG-CONTEXT FACTUALITY
1. 指出一些attention head可能和基于transformer的LLM对长文本中特定文本的获取有关。就是能通过上下文获取到关键信息