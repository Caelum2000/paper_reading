## Never Train from Scratch: FAIR COMPARISON OF LONG-
1. 证明transformer在self-pretrain之后比其他的long seqence模型强
2. self-pretraining(SPT) 是指mask或在denoise进行pretrain
3. 主要比较的模型是state space model (SSM), 具体SSM是什么还需要调研