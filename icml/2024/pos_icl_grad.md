## Position: Do pretrained Transformers Learn In-Context by Gradient Descent?
1. 研究LLM ICL的能力和梯度下降的关系
2. 是根据之前一系列ICL和grad descent工作的关系的研究展开的
3. 对之前工作提出的ICL和grad的等价关系提出了质疑