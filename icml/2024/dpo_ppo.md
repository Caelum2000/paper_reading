## Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
1. 实验表明DPO表现不如PPO好。DPO也可能发生reward hacking, DPO求解空间比PPO大
2. DPO会给一些没有在perference dataset出现的回答给大的概率
3. 本文的论点有点离散，没有很理论化的讲清楚DPO不好的原因，实验支撑比较多