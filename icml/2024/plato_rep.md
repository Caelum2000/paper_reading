## Position: The Platonic Representation Hypothesis
1. 观测到机器学习的表征收敛到同一个空间或形式（当然，这个存疑）
2. 包括跨模态，不同模型，不同任务等
3. 这个收敛的空间有点类似柏拉图的“理念”
4. 解释了一部分原因，没细看，回过头再看


### review
1. 假设0：对事物概念的表征收敛到同一个表示空间
2. 假设1（原因）：多任务的训练方式会导致假设0成立，因为要对多任务建立一个共同的表征，因此特征空间的求解域就减小，从而收敛到generalized representaion
3. 假设2（原因）：模型的规模增大会导致假设0成立，理由是模型规模增大可行域更大，更容易求解到这种generalized representation
4. 假设3（原因）：使模型简化的正则化约束更有可能导致假设0成立，例如L2正则化，使权重更稀疏，更容易找到“门道”或“机制”，而不是捕捉虚假的相关性，导致过拟合
5. 从上述关于原因的假设看来，增大模型规模不一定是好事，同时要注重泛化能力和训练方式（如多任务，正则化）